[{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using the `sparseR` package","text":"sparseR package aids implementation ranked sparsity methods model selection presence interactions polynomials. following methods supported, help ncvreg backend: sparsity-ranked lasso (SRL) Sparsity-ranked MCP/SCAD (SRM/SRS) Sparsity-ranked elastic net (SREN) Additionally, sparseR makes easy preprocess data get ready looking candidate interactions order k, possible polynomials order poly. document goes several use cases package different datasets.","code":""},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"use-case-iris-data","dir":"Articles","previous_headings":"","what":"Use case: iris data","title":"Using the `sparseR` package","text":"illustrate simple case, consider Fisher’s Iris data set, composed 4 numeric variables one categorical variable. interested predicting Sepal.Width based variables pairwise interactions, couple options. can either use built-preprocessing building model matrix fitting model, can manually build model matrix pass sparseR function.","code":"data(iris) summary(iris) ##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    ##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   ##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   ##  Median :5.800   Median :3.000   Median :4.350   Median :1.300   ##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199   ##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800   ##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500   ##        Species   ##  setosa    :50   ##  versicolor:50   ##  virginica :50   ##                  ##                  ##"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"using-formula-specification","dir":"Articles","previous_headings":"Use case: iris data","what":"Using formula specification","title":"Using the `sparseR` package","text":"Passing formula sparseR tell function main effects model. also main effects indicate interactions/polynomials investigated. Note formula accept main effect terms, allow specification individual interaction effects (functions main effects). sparsity-ranked lasso model -pairwise interactions can fit follows (cross-validation performed lambda via ncvreg package, important set seed reproducibililty). print method present useful results fits: detailed model information, summary method sparseR objects displays information fits (see ?ncvreg::summary.ncvreg information). Finally, plots either cross-validation error across lambda values coefficient paths can produced fairly easily:   Importantly, since sparseR formula centers scales covariates prior forming interactions, coefficients interpreted way (1-unit change covariate corresponds 1 SD change unit main effect, parameters involving interactions must interpreted mean value involved covariates).","code":"srl <- sparseR(Sepal.Width ~ ., data = iris, k = 1, seed = 1) srl ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=150, p=18 ##   (At lambda=0.0015): ##     Nonzero coefficients: 10 ##     Cross-validation error (deviance): 0.07 ##     R-squared: 0.62 ##     Signal-to-noise ratio: 1.64 ##     Scale estimate (sigma): 0.267 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect     6        4      0.667    2.45 ##  Order 1 interaction    12        6      0.500    3.46 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=150, p=18 ##   (At lambda=0.0070): ##     Nonzero coefficients: 7 ##     Cross-validation error (deviance): 0.08 ##     R-squared: 0.57 ##     Signal-to-noise ratio: 1.33 ##     Scale estimate (sigma): 0.285 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect     6        3      0.500    2.45 ##  Order 1 interaction    12        4      0.333    3.46 # At the lambda which minimizes CVE summary(srl, at = \"cvmin\") ## Using a basic kernel estimate for local fdr; consider installing the ashr package for more accurate estimation.  See ?local_mfdr ## lasso-penalized linear regression with n=150, p=18 ## At lambda=0.0015: ## ------------------------------------------------- ##   Nonzero coefficients         :  10 ##   Expected nonzero coefficients:   3.36 ##   Average mfdr (10 features)   :   0.336 ##  ##                                 Estimate       z       mfdr Selected ## Species_setosa                   1.24843 28.8653    < 1e-04        * ## Sepal.Length:Species_setosa      0.35020  9.3864    < 1e-04        * ## Sepal.Length                     0.18424  9.1304    < 1e-04        * ## Petal.Width:Species_virginica    0.23716  6.6223    < 1e-04        * ## Petal.Width:Species_versicolor   0.55330  4.7733 0.00017238        * ## Species_virginica               -0.09639 -2.3907 0.27557456        * ## Sepal.Length:Petal.Length       -0.04447 -2.2272 0.36889592        * ## Petal.Length:Petal.Width         0.04928  2.0135 0.71570426        * ## Petal.Length                    -0.02042 -1.1660 1.00000000        * ## Sepal.Length:Species_versicolor -0.01629 -0.5358 1.00000000        * # At the lambda which is within 1 SE of the minimum CVE summary(srl, at = \"cv1se\") ## lasso-penalized linear regression with n=150, p=18 ## At lambda=0.0070: ## ------------------------------------------------- ##   Nonzero coefficients         :   7 ##   Expected nonzero coefficients:   1.38 ##   Average mfdr (7 features)    :   0.198 ##  ##                                 Estimate       z     mfdr Selected ## Species_setosa                  0.810513 17.9513  < 1e-04        * ## Sepal.Length                    0.191210  9.3371  < 1e-04        * ## Petal.Length:Petal.Width        0.119640  5.0379  < 1e-04        * ## Petal.Width:Species_versicolor  0.275341  3.1640 0.055680        * ## Sepal.Length:Petal.Length      -0.052711 -3.2466 0.078121        * ## Sepal.Length:Species_setosa     0.062782  2.5978 0.251076        * ## Species_versicolor             -0.001653 -0.8052 1.000000        * plot(srl, plot_type = \"cv\") plot(srl, plot_type = \"path\")"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"using-model-matrix-specification","dir":"Articles","previous_headings":"Use case: iris data","what":"Using model matrix specification","title":"Using the `sparseR` package","text":"Since sparseR preprocessing limits customizability, often interest create interactions centered prespecified values (opposed mean), also possible feed matrix sparseR already preprocessed. function also allows users circumvent recipes functionality supply model matrix, composed interactions /polynomials. may useful cases users wish : specify missing data imputation methods, specify intercept differently grand mean (avoid centering variables prior making certain interactions interest) scale variables differently key arguments purpose set pre_process FALSE, supply model_matrix outcome y. Users may also want specify model matrix specified terms polynomial order (package requires prefix, “poly_1”, “poly_2” denote polynomial terms orders 1 2 respectively) interaction seperator (default, “\\\\:”, default model.matrix). code creates model matrix pairwise interactions, however opposed formula, numeric variables centered prior forming interactions, factor variable species given reference category (opposed cell-means specification). Therefore, method lead different results. Since warning iterations, note can pass arguments ncvreg fix issue: can use S3 methods get information SRL model:  Since center/scale variables prior fitting procedure, interpretation coefficients original scale predictors, origin terms involved interactions 0, mean constituent covariates. Note also fewer total covariates listed. model.matrix uses reference cell dummy variables default. Another way without reference category specification pass pre-process options sparseR, manually use sparseR_prep create model matrix.","code":"X <- model.matrix(Sepal.Width ~ .*., data = iris)[,-1] set.seed(1) srl2 <- sparseR(pre_process = FALSE, model_matrix = X, y = iris$Sepal.Width) ## Warning in ncvreg(X = X, y = y, ...): Maximum number of iterations reached set.seed(1) srl2 <- sparseR(pre_process = FALSE, model_matrix = X, y = iris$Sepal.Width, max.iter = 1e6) srl2 ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=150, p=14 ##   (At lambda=0.0005): ##     Nonzero coefficients: 8 ##     Cross-validation error (deviance): 0.07 ##     R-squared: 0.64 ##     Signal-to-noise ratio: 1.77 ##     Scale estimate (sigma): 0.261 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect     5        3      0.600    2.24 ##  Order 1 interaction     9        5      0.556    3.00 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=150, p=14 ##   (At lambda=0.0021): ##     Nonzero coefficients: 5 ##     Cross-validation error (deviance): 0.08 ##     R-squared: 0.59 ##     Signal-to-noise ratio: 1.46 ##     Scale estimate (sigma): 0.277 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect     5        4      0.800    2.24 ##  Order 1 interaction     9        1      0.111    3.00 summary(srl2, at = \"cv1se\") ## lasso-penalized linear regression with n=150, p=14 ## At lambda=0.0021: ## ------------------------------------------------- ##   Nonzero coefficients         :   5 ##   Expected nonzero coefficients:   0.00 ##   Average mfdr (5 features)    :   0.000 ##  ##                           Estimate      z    mfdr Selected ## Speciesvirginica           -1.1502 -25.15 < 1e-04        * ## Speciesversicolor          -1.0558 -23.10 < 1e-04        * ## Sepal.Length                0.4243  16.32 < 1e-04        * ## Petal.Width                 0.3390  12.06 < 1e-04        * ## Sepal.Length:Petal.Length  -0.0189 -11.68 < 1e-04        * plot(srl2) set.seed(1)  srl3 <- sparseR(Sepal.Width ~ ., data = iris, k = 1,                  pre_proc_opts = c(\"none\"), max.iter = 1e6) srl3 ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=150, p=18 ##   (At lambda=0.0007): ##     Nonzero coefficients: 6 ##     Cross-validation error (deviance): 0.07 ##     R-squared: 0.64 ##     Signal-to-noise ratio: 1.74 ##     Scale estimate (sigma): 0.262 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect     6        2      0.333    2.45 ##  Order 1 interaction    12        4      0.333    3.46 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=150, p=18 ##   (At lambda=0.0041): ##     Nonzero coefficients: 5 ##     Cross-validation error (deviance): 0.08 ##     R-squared: 0.59 ##     Signal-to-noise ratio: 1.42 ##     Scale estimate (sigma): 0.279 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect     6        3      0.500    2.45 ##  Order 1 interaction    12        2      0.167    3.46 summary(srl3, at = \"cv1se\") ## lasso-penalized linear regression with n=150, p=18 ## At lambda=0.0041: ## ------------------------------------------------- ##   Nonzero coefficients         :   5 ##   Expected nonzero coefficients:   1.82 ##   Average mfdr (5 features)    :   0.364 ##  ##                              Estimate       z    mfdr Selected ## Sepal.Length:Species_setosa  0.211340 23.4596 < 1e-04        * ## Sepal.Length                 0.233045  9.2221 < 1e-04        * ## Petal.Width                  0.179185  6.6622 < 1e-04        * ## Petal.Width:Species_setosa   0.002791  0.6622 0.81825        * ## Species_virginica           -0.056953 -1.6796 1.00000        *"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"centering-to-different-values","dir":"Articles","previous_headings":"Use case: iris data","what":"Centering to different values","title":"Using the `sparseR` package","text":"many cases interactions, makes sense center variables certain values (rather, make sense center variables mean zero). Therefore built flexibility sparseR sparseR_prep order allow users change context dictates. covariate’s centering location (case minima) can passed sparseR_prep follows: directly sparseR: , function can passed evaluate location covariate:","code":"cc <- iris %>%     select(Sepal.Length, Petal.Length, Petal.Width) %>%     apply(2, min, na.rm = TRUE)   p1 <- sparseR_prep(Sepal.Width ~ ., iris, k = 0, extra_opts = list(centers = cc)) (c2min <- bake(p1, iris)) ## # A tibble: 150 × 6 ##    Sepal.Length Petal.Length Petal.Width Species_setosa Species_versicolor ##           <dbl>        <dbl>       <dbl>          <dbl>              <dbl> ##  1        0.966        0.227       0.131              1                  0 ##  2        0.725        0.227       0.131              1                  0 ##  3        0.483        0.170       0.131              1                  0 ##  4        0.362        0.283       0.131              1                  0 ##  5        0.845        0.227       0.131              1                  0 ##  6        1.33         0.397       0.394              1                  0 ##  7        0.362        0.227       0.262              1                  0 ##  8        0.845        0.283       0.131              1                  0 ##  9        0.121        0.227       0.131              1                  0 ## 10        0.725        0.283       0                  1                  0 ## # ℹ 140 more rows ## # ℹ 1 more variable: Species_virginica <dbl> summary(c2min) ##   Sepal.Length     Petal.Length     Petal.Width     Species_setosa   ##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   ##  1st Qu.:0.9661   1st Qu.:0.3399   1st Qu.:0.2624   1st Qu.:0.0000   ##  Median :1.8115   Median :1.8977   Median :1.5743   Median :0.0000   ##  Mean   :1.8638   Mean   :1.5623   Mean   :1.4422   Mean   :0.3333   ##  3rd Qu.:2.5360   3rd Qu.:2.3226   3rd Qu.:2.2303   3rd Qu.:1.0000   ##  Max.   :4.3475   Max.   :3.3422   Max.   :3.1486   Max.   :1.0000   ##  Species_versicolor Species_virginica ##  Min.   :0.0000     Min.   :0.0000    ##  1st Qu.:0.0000     1st Qu.:0.0000    ##  Median :0.0000     Median :0.0000    ##  Mean   :0.3333     Mean   :0.3333    ##  3rd Qu.:1.0000     3rd Qu.:1.0000    ##  Max.   :1.0000     Max.   :1.0000 srl_centered2min <- sparseR(Sepal.Width ~ ., iris, extra_opts = list(centers = cc), seed = 1) p2 <- sparseR_prep(Sepal.Width ~ ., iris, k = 0, extra_opts = list(center_fn = min)) (c2min2 <- bake(p2, iris)) ## # A tibble: 150 × 6 ##    Sepal.Length Petal.Length Petal.Width Species_setosa Species_versicolor ##           <dbl>        <dbl>       <dbl>          <dbl>              <dbl> ##  1        0.966        0.227       0.131              1                  0 ##  2        0.725        0.227       0.131              1                  0 ##  3        0.483        0.170       0.131              1                  0 ##  4        0.362        0.283       0.131              1                  0 ##  5        0.845        0.227       0.131              1                  0 ##  6        1.33         0.397       0.394              1                  0 ##  7        0.362        0.227       0.262              1                  0 ##  8        0.845        0.283       0.131              1                  0 ##  9        0.121        0.227       0.131              1                  0 ## 10        0.725        0.283       0                  1                  0 ## # ℹ 140 more rows ## # ℹ 1 more variable: Species_virginica <dbl> identical(c2min2, c2min) ## [1] TRUE"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"plotting-the-model","dir":"Articles","previous_headings":"Use case: iris data","what":"Plotting the model","title":"Using the `sparseR` package","text":"Interactions notoriously difficult understand communicate, visualizations can often helpful. sparseR package limited functionality plotting results fits, using visreg package rough guide.  Notice changes covariates centered minima instead:  ’s clear model can change quite bit depending center location utilized prior building interactions. Unfortunately, optimizing center location covariate difficult, p-dimensional problem (set context another difficult optimization problem regularized regression multiple tuning parameters). sparseR package allow users compare certain choices terms cross-validated error, decent starting point. plot shows “centered zero” model slightly lower cross-validated error best mean-centered model best minimum-centered model.  Though models look substantially different, ’s worth noting predictions obtained three models close one another (since models look similar neighborhood existing data):  CV1se lambda value, slightly less agreement predictions:","code":"effect_plot(srl, \"Petal.Width\", by = \"Species\") effect_plot(srl_centered2min, \"Petal.Width\", by = \"Species\") plot(srl3, plot_type = \"cv\", ylim = c(0,.2)) abline(h = min(srl3$fit$cve), col = \"red\") plot(srl_centered2min, plot_type = \"cv\", ylim = c(0,.2)) abline(h = min(srl3$fit$cve), col = \"red\") plot(srl, plot_type = \"cv\", ylim = c(0,.2)) abline(h = min(srl3$fit$cve), col = \"red\")  effect_plot(srl3, \"Petal.Width\", by = \"Species\",              plot.args = list(ylim = c(1.5, 4.8))) effect_plot(srl_centered2min, \"Petal.Width\", by = \"Species\",              plot.args = list(ylim = c(1.5, 4.8))) effect_plot(srl, \"Petal.Width\", by = \"Species\",              plot.args = list(ylim = c(1.5, 4.8))) p1 <- predict(srl3, at = \"cvmin\") p2 <- predict(srl_centered2min, at = \"cvmin\") p3 <- predict(srl, at = \"cvmin\")  cor(cbind(p1, p2 ,p3)) ##           p1        p2        p3 ## p1 1.0000000 0.9919068 0.9919581 ## p2 0.9919068 1.0000000 0.9946687 ## p3 0.9919581 0.9946687 1.0000000 pairs(cbind(p1, p2 ,p3)) # At CV1se p4 <- predict(srl3, at = \"cv1se\") p5 <- predict(srl_centered2min, at = \"cv1se\") p6 <- predict(srl, at = \"cv1se\")  cor(cbind(p1, p2 ,p3, p4,p5,p6)) ##           p1        p2        p3        p4        p5        p6 ## p1 1.0000000 0.9919068 0.9919581 0.9761025 0.9607660 0.9744660 ## p2 0.9919068 1.0000000 0.9946687 0.9718119 0.9797246 0.9642979 ## p3 0.9919581 0.9946687 1.0000000 0.9704648 0.9701393 0.9763341 ## p4 0.9761025 0.9718119 0.9704648 1.0000000 0.9787516 0.9891270 ## p5 0.9607660 0.9797246 0.9701393 0.9787516 1.0000000 0.9602063 ## p6 0.9744660 0.9642979 0.9763341 0.9891270 0.9602063 1.0000000 pairs(cbind(p1, p2 ,p3, p4,p5,p6)) effect_plot(srl3, \"Petal.Width\", by = \"Species\", at = \"cv1se\") effect_plot(srl_centered2min, \"Petal.Width\", by = \"Species\", at = \"cv1se\") effect_plot(srl, \"Petal.Width\", by = \"Species\", at = \"cv1se\")"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"using-stepwise-rbic","dir":"Articles","previous_headings":"Use case: iris data","what":"Using stepwise RBIC","title":"Using the `sparseR` package","text":"RBIC, Ranked-sparsity Bayesian Information Criterion, ranked sparsity model selection criterion correctly penalize interactions (higher main effects). benefit using stepwise approach informed RBIC shrinkage, thus doesn’t matter whether () covariates centered (usually). exceptions occur stepwise process sequential/hierarchical (.e. main effects first, interactions “active” main effects, etc.). Inference parameters still ) difficult one must account post-selection inference, ii) affected whether covariates centered. However, lines affected centering status. can see examples :  similar S3 methods available (note inference change certain parameters based centering status). message produced p-values account post-selection inference. Alternatively, sample splitting can utilized achieve valid inference. method divides data two samples equal size, uses first sample select model, second one fit achieve valid (though potentially low-power) inference. Coefficients selected bestowed p-value 1. Additionally, inferences based bootstrapped samples data available via sparseRBIC_bootstrap function, stores p-values coefficient model selection performed. coefficient selected, p-value set one bootstrap iteration, “mean” p-value calculated across bootstraps, underestimation p-value due model selection attenuated, estimate gets “pulled ” 1’s vector. Since may conservative approach, also display “geometric” mean measure central tendency less sensitive skew (many p-values highly right skewed). research must done techniques ensure adequate coverage properties, given high enough number bootstrap samples conservative naive, “pretend model selection never took place” approach. downside depending size data number variables, take quite time. Note mean p-value across bootstraps bounded greater 1-P(selected). sense, makes sense, variable low probability selected, variable higher adjusted p-value. However, variables whose effects highly related (.e. interaction constituent main effect), may conservative. section still development.","code":"## Centered model (rbic1 <- sparseRBIC_step(Sepal.Width ~ ., iris, pre_proc_opts = c(\"center\", \"scale\"))) ## Note: sparseRBIC_step is currently experimental and may not behave as expected. ##  ## Call:  glm(formula = y ~ Species_setosa + Sepal.Length + Petal.Width +  ##     Species_versicolor + Petal.Length + `Sepal.Length:Species_setosa`,  ##     family = family, data = X) ##  ## Coefficients: ##                   (Intercept)                 Species_setosa   ##                       2.40062                        2.14130   ##                  Sepal.Length                    Petal.Width   ##                       0.16506                        0.44726   ##            Species_versicolor                   Petal.Length   ##                       0.29340                       -0.05916   ## `Sepal.Length:Species_setosa`   ##                       0.45943   ##  ## Degrees of Freedom: 149 Total (i.e. Null);  143 Residual ## Null Deviance:       28.31  ## Residual Deviance: 8.999     AIC: 19.65 # Non-centered model (rbic2 <- sparseRBIC_step(Sepal.Width ~ ., iris, pre_proc_opts = c(\"scale\"))) ## Note: sparseRBIC_step is currently experimental and may not behave as expected. ##  ## Call:  glm(formula = y ~ Species_setosa + Sepal.Length + Petal.Width +  ##     Species_versicolor + Petal.Length + `Sepal.Length:Species_setosa`,  ##     family = family, data = X) ##  ## Coefficients: ##                   (Intercept)                 Species_setosa   ##                       0.65809                       -1.10073   ##                  Sepal.Length                    Petal.Width   ##                       0.16506                        0.44726   ##            Species_versicolor                   Petal.Length   ##                       0.29340                       -0.05916   ## `Sepal.Length:Species_setosa`   ##                       0.45943   ##  ## Degrees of Freedom: 149 Total (i.e. Null);  143 Residual ## Null Deviance:       28.31  ## Residual Deviance: 8.999     AIC: 19.65 effect_plot(rbic1, \"Petal.Width\", by = \"Species\", plot.args = list(ylim = c(1.5, 5))) effect_plot(rbic2, \"Petal.Width\", by = \"Species\", plot.args = list(ylim = c(1.5, 5))) effect_plot(rbic1, \"Sepal.Length\", by = \"Species\") effect_plot(rbic2, \"Sepal.Length\", by = \"Species\") summary(rbic1) ## P-values have **not** been corrected for multiple comparisons ## Consider sparseRBIC_sampsplit() or sparseRBIC_bootstrap() ##  ## Call: ## glm(formula = y ~ Species_setosa + Sepal.Length + Petal.Width +  ##     Species_versicolor + Petal.Length + `Sepal.Length:Species_setosa`,  ##     family = family, data = X) ##  ## Deviance Residuals:  ##      Min        1Q    Median        3Q       Max   ## -0.78351  -0.15819   0.01874   0.16809   0.61264   ##  ## Coefficients: ##                               Estimate Std. Error t value Pr(>|t|)     ## (Intercept)                    2.40062    0.12025  19.964  < 2e-16 *** ## Species_setosa                 2.14130    0.30584   7.001 9.10e-11 *** ## Sepal.Length                   0.16506    0.06017   2.743  0.00687 **  ## Petal.Width                    0.44726    0.08833   5.064 1.25e-06 *** ## Species_versicolor             0.29340    0.09839   2.982  0.00337 **  ## Petal.Length                  -0.05916    0.15021  -0.394  0.69428     ## `Sepal.Length:Species_setosa`  0.45943    0.09997   4.596 9.40e-06 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for gaussian family taken to be 0.06292687) ##  ##     Null deviance: 28.3069  on 149  degrees of freedom ## Residual deviance:  8.9985  on 143  degrees of freedom ## AIC: 19.646 ##  ## Number of Fisher Scoring iterations: 2 summary(rbic2) ## P-values have **not** been corrected for multiple comparisons ## Consider sparseRBIC_sampsplit() or sparseRBIC_bootstrap() ##  ## Call: ## glm(formula = y ~ Species_setosa + Sepal.Length + Petal.Width +  ##     Species_versicolor + Petal.Length + `Sepal.Length:Species_setosa`,  ##     family = family, data = X) ##  ## Deviance Residuals:  ##      Min        1Q    Median        3Q       Max   ## -0.78351  -0.15819   0.01874   0.16809   0.61264   ##  ## Coefficients: ##                               Estimate Std. Error t value Pr(>|t|)     ## (Intercept)                    0.65809    0.31114   2.115  0.03616 *   ## Species_setosa                -1.10073    0.60258  -1.827  0.06983 .   ## Sepal.Length                   0.16506    0.06017   2.743  0.00687 **  ## Petal.Width                    0.44726    0.08833   5.064 1.25e-06 *** ## Species_versicolor             0.29340    0.09839   2.982  0.00337 **  ## Petal.Length                  -0.05916    0.15021  -0.394  0.69428     ## `Sepal.Length:Species_setosa`  0.45943    0.09997   4.596 9.40e-06 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for gaussian family taken to be 0.06292687) ##  ##     Null deviance: 28.3069  on 149  degrees of freedom ## Residual deviance:  8.9985  on 143  degrees of freedom ## AIC: 19.646 ##  ## Number of Fisher Scoring iterations: 2 s1 <- sparseRBIC_sampsplit(rbic1) ## Note: sparseRBIC_sampsplit is currently experimental and may not behave as expected. ## Warning: `progress_estimated()` was deprecated in dplyr 1.0.0. ## ℹ The deprecated feature was likely used in the sparseR package. ##   Please report the issue to the authors. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. s2 <- sparseRBIC_sampsplit(rbic2) ## Note: sparseRBIC_sampsplit is currently experimental and may not behave as expected. set.seed(1) ## Centered model b1 <- sparseRBIC_bootstrap(rbic1) set.seed(1) ## Uncentered model b2 <- sparseRBIC_bootstrap(rbic2) ## Note: sparseRBIC_bootstrap is currently experimental and may not behave as expected."},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"use-case-lung-data","dir":"Articles","previous_headings":"","what":"Use case: lung data","title":"Using the `sparseR` package","text":"realistic example, take data set often used train models can predict lung cancer status. 1027 observations 14 covariates, including urban/rural, age, years school, smoking years, number children, etc. First, let’s look data little: Note: missing values BMI - see sparseR handles . Everything coded numeric except outcome, case (necessary sparseR work), see. ID arbitrary identifying variable used modeling phase point, split data test training set.","code":"data(\"irlcs_radon_syn\") summary(irlcs_radon_syn) ##        ID         CITY       YRSHOME          AGE        CASE    SCHOOL  ##  Min.   :   1.0   0:246   Min.   :20.0   Min.   :44.16   0:632   1: 92   ##  1st Qu.: 260.5   1:781   1st Qu.:25.0   1st Qu.:61.38   1:395   2:506   ##  Median : 492.0           Median :31.0   Median :67.84           3:293   ##  Mean   : 505.2           Mean   :33.1   Mean   :67.45           4:106   ##  3rd Qu.: 761.0           3rd Qu.:39.0   3rd Qu.:73.82           5: 30   ##  Max.   :1026.0           Max.   :75.0   Max.   :84.80                   ##                                                                          ##      SMKYRS         CHILDREN           PYR            PYRRATE        ##  Min.   : 0.00   Min.   : 0.000   Min.   :  0.00   Min.   :0.00000   ##  1st Qu.: 0.00   1st Qu.: 2.000   1st Qu.:  0.00   1st Qu.:0.00000   ##  Median :13.00   Median : 3.000   Median :  3.60   Median :0.05368   ##  Mean   :20.58   Mean   : 3.065   Mean   : 19.31   Mean   :0.31745   ##  3rd Qu.:41.00   3rd Qu.: 4.000   3rd Qu.: 35.07   3rd Qu.:0.58024   ##  Max.   :64.00   Max.   :13.000   Max.   :138.45   Max.   :2.55702   ##                                                                      ##     SMKQUIT            BMI            WLM20        PRELUNG SMKEVER SMKCUR  ##  Min.   : 0.000   Min.   :16.64   Min.   : 2.176   0:692   0:476   0:705   ##  1st Qu.: 0.000   1st Qu.:21.79   1st Qu.: 5.902   1:335   1:551   1:322   ##  Median : 0.000   Median :23.78   Median : 8.683                           ##  Mean   : 4.733   Mean   :24.60   Mean   :10.914                           ##  3rd Qu.: 3.325   3rd Qu.:27.40   3rd Qu.:13.246                           ##  Max.   :57.355   Max.   :41.60   Max.   :77.457                           ##                   NA's   :51 irlcs_radon_syn <- select(irlcs_radon_syn, -ID) set.seed(13)  N <- nrow(irlcs_radon_syn) trainIDX <- sample(1:N, N * .75) train <- irlcs_radon_syn[trainIDX,] test <- irlcs_radon_syn[-trainIDX,]"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"preprocessing","dir":"Articles","previous_headings":"Use case: lung data","what":"Preprocessing","title":"Using the `sparseR` package","text":"Although sparseR() function automatically, ’s generally good idea see data preprocessed examine model matrix built regularization. can see sparseR preprocess data running function sparseR_prep, utilizes recipes package perform preprocessing. time outcome used train steps, except elimination NA values exist. see preprocessor many things, among imputed data missing value using predictors model. Apparently, default, preprocessor remove variables “near” zero variance, result, SMKYRS considered (’s eliminated early step). look variable, see may informative:  Therefore, can tell preprocessor filter variable two ways. First, tell use zv instead nzv, case remove zero variance predictors: , adjust options near-zero variance filter (see ?recipes::step_nzv details): Notice haven’t told preprocessor build interactions polynomials higher degree 1. can simply setting k poly respectively higher numbers: Note: preprocessor actually produce data, prepares steps. recipes package, known prepping processor. final step called bake, data fed processor new data set returned.","code":"prep_obj <- sparseR_prep(CASE ~ ., data = train, k = 0, poly = 1) prep_obj ## ## ── Recipe ────────────────────────────────────────────────────────────────────── ## ## ── Inputs ## Number of variables by role ## outcome:    1 ## predictor: 10 ## dummy:      4 ## ## ── Training information ## Training data contained 770 data points and 43 incomplete rows. ## ## ── Operations ## • Variables removed: <none> | Trained ## • Sparse, unbalanced variable filter removed: <none> | Trained ## Centering to mean for YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, S... [trained] ## • Scaling for: YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, ... | Trained ## • Collapsing factor levels for: SCHOOL | Trained ## • K-nearest neighbor imputation for: YRSHOME, AGE, SCHOOL, ... | Trained ## • Removing rows with NA values in: CITY, YRSHOME, AGE, SCHOOL, ... | Trained ## • Variables removed: CASE | Trained ## • Dummy variables from: CITY, SCHOOL, PRELUNG, SMKEVER, SMKCUR | Trained ## • Sparse, unbalanced variable filter removed: SCHOOL_other | Trained MASS::truehist(train$SMKYRS) sparseR_prep(CASE ~ ., data = train, k = 0, poly = 1, filter = \"zv\") ## ## ── Recipe ────────────────────────────────────────────────────────────────────── ## ## ── Inputs ## Number of variables by role ## outcome:    1 ## predictor: 10 ## dummy:      4 ## ## ── Training information ## Training data contained 770 data points and 43 incomplete rows. ## ## ── Operations ## • Variables removed: <none> | Trained ## • Zero variance filter removed: <none> | Trained ## Centering to mean for YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, S... [trained] ## • Scaling for: YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, ... | Trained ## • Collapsing factor levels for: SCHOOL | Trained ## • K-nearest neighbor imputation for: YRSHOME, AGE, SCHOOL, ... | Trained ## • Removing rows with NA values in: CITY, YRSHOME, AGE, SCHOOL, ... | Trained ## • Variables removed: CASE | Trained ## • Dummy variables from: CITY, SCHOOL, PRELUNG, SMKEVER, SMKCUR | Trained ## • Zero variance filter removed: <none> | Trained sparseR_prep(CASE ~ ., data = train, k = 0, poly = 1, extra_opts = list(unique_cut = 5)) ## ## ── Recipe ────────────────────────────────────────────────────────────────────── ## ## ── Inputs ## Number of variables by role ## outcome:    1 ## predictor: 10 ## dummy:      4 ## ## ── Training information ## Training data contained 770 data points and 43 incomplete rows. ## ## ── Operations ## • Variables removed: <none> | Trained ## • Sparse, unbalanced variable filter removed: <none> | Trained ## Centering to mean for YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, S... [trained] ## • Scaling for: YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, ... | Trained ## • Collapsing factor levels for: SCHOOL | Trained ## • K-nearest neighbor imputation for: YRSHOME, AGE, SCHOOL, ... | Trained ## • Removing rows with NA values in: CITY, YRSHOME, AGE, SCHOOL, ... | Trained ## • Variables removed: CASE | Trained ## • Dummy variables from: CITY, SCHOOL, PRELUNG, SMKEVER, SMKCUR | Trained ## • Sparse, unbalanced variable filter removed: SCHOOL_other | Trained sparseR_prep(CASE ~ ., data = train, k = 1, poly = 1, extra_opts = list(unique_cut = 5)) ## ## ── Recipe ────────────────────────────────────────────────────────────────────── ## ## ── Inputs ## Number of variables by role ## outcome:    1 ## predictor: 10 ## dummy:      4 ## ## ── Training information ## Training data contained 770 data points and 43 incomplete rows. ## ## ── Operations ## • Variables removed: <none> | Trained ## • Sparse, unbalanced variable filter removed: <none> | Trained ## Centering to mean for YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, S... [trained] ## • Scaling for: YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, ... | Trained ## • Collapsing factor levels for: SCHOOL | Trained ## • K-nearest neighbor imputation for: YRSHOME, AGE, SCHOOL, ... | Trained ## • Removing rows with NA values in: CITY, YRSHOME, AGE, SCHOOL, ... | Trained ## • Variables removed: CASE | Trained ## • Dummy variables from: CITY, SCHOOL, PRELUNG, SMKEVER, SMKCUR | Trained ## • Interactions with: (YRSHOME + AGE + SMKYRS + CHILDREN + PYR + PYRRATE + ##   SMKQUIT + BMI + WLM20 + (CITY_X0 + CITY_X1 + SCHOOL_X1 + SCHOOL_X2 + ##   SCHOOL_X3 + SCHOOL_X4 + SCHOOL_other + PRELUNG_X0 + PRELUNG_X1 + SMKEVER_X0 + ##   SMKEVER_X1 + SMKCUR_X0 + SMKCUR_X1)) * (YRSHOME + AGE + SMKYRS + CHILDREN + ##   PYR + PYRRATE + SMKQUIT + BMI + WLM20 + (CITY_X0 + CITY_X1 + SCHOOL_X1 + ##   SCHOOL_X2 + SCHOOL_X3 + SCHOOL_X4 + SCHOOL_other + PRELUNG_X0 + PRELUNG_X1 + ##   SMKEVER_X0 + SMKEVER_X1 + SMKCUR_X0 + SMKCUR_X1)) | Trained ## • Sparse, unbalanced variable filter removed: SCHOOL_other, ... | Trained sparseR_prep(CASE ~ ., data = train, k = 1, poly = 2, extra_opts = list(unique_cut = 5)) ## ## ── Recipe ────────────────────────────────────────────────────────────────────── ## ## ── Inputs ## Number of variables by role ## outcome:    1 ## predictor: 10 ## dummy:      4 ## ## ── Training information ## Training data contained 770 data points and 43 incomplete rows. ## ## ── Operations ## • Variables removed: <none> | Trained ## • Sparse, unbalanced variable filter removed: <none> | Trained ## Centering to mean for YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, S... [trained] ## • Scaling for: YRSHOME, AGE, SMKYRS, CHILDREN, PYR, PYRRATE, ... | Trained ## • Collapsing factor levels for: SCHOOL | Trained ## • K-nearest neighbor imputation for: YRSHOME, AGE, SCHOOL, ... | Trained ## • Removing rows with NA values in: CITY, YRSHOME, AGE, SCHOOL, ... | Trained ## • Variables removed: CASE | Trained ## • Dummy variables from: CITY, SCHOOL, PRELUNG, SMKEVER, SMKCUR | Trained ## • Interactions with: (YRSHOME + AGE + SMKYRS + CHILDREN + PYR + PYRRATE + ##   SMKQUIT + BMI + WLM20 + (CITY_X0 + CITY_X1 + SCHOOL_X1 + SCHOOL_X2 + ##   SCHOOL_X3 + SCHOOL_X4 + SCHOOL_other + PRELUNG_X0 + PRELUNG_X1 + SMKEVER_X0 + ##   SMKEVER_X1 + SMKCUR_X0 + SMKCUR_X1)) * (YRSHOME + AGE + SMKYRS + CHILDREN + ##   PYR + PYRRATE + SMKQUIT + BMI + WLM20 + (CITY_X0 + CITY_X1 + SCHOOL_X1 + ##   SCHOOL_X2 + SCHOOL_X3 + SCHOOL_X4 + SCHOOL_other + PRELUNG_X0 + PRELUNG_X1 + ##   SMKEVER_X0 + SMKEVER_X1 + SMKCUR_X0 + SMKCUR_X1)) | Trained ## • Orthogonal polynomials on: YRSHOME, AGE, SMKYRS, CHILDREN, PYR, ... | Trained ## • Sparse, unbalanced variable filter removed: SCHOOL_other, ... | Trained"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"using-the-sparser-function","dir":"Articles","previous_headings":"Use case: lung data","what":"Using the sparseR function","title":"Using the `sparseR` package","text":"preprocessing options can accomplished main function package, sparseR. following code runs SRL model, APL model, main effects model, SRL polynomials: simple print method allows one investigate models Similarly, plots can produced figures:  MCP:  SCAD (evaluated): Summaries (using results_summary results_1se_summary values within object; formatting omitted).","code":"lso <- list(   SRL  = sparseR(CASE ~ ., train, seed = 1),            ## SRL model   APL  = sparseR(CASE ~ ., train, seed = 1, gamma = 0), ## APL model   ME   = sparseR(CASE ~ ., train, seed = 1, k = 0),     ## Main effects model   SRLp = sparseR(CASE ~ ., train, seed = 1, poly = 2)   ## SRL + polynomials ) lso ## $SRL ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=195 ##   (At lambda=0.0007): ##     Nonzero coefficients: 42 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.43 ##     Signal-to-noise ratio: 0.74 ##     Scale estimate (sigma): 0.368 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    21       10      0.476    4.58 ##  Order 1 interaction   174       32      0.184   13.19 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=195 ##   (At lambda=0.0098): ##     Nonzero coefficients: 4 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.40 ##     Signal-to-noise ratio: 0.65 ##     Scale estimate (sigma): 0.378 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    21        4       0.19    4.58 ##  Order 1 interaction   174        0       0.00   13.19 ##  ## $APL ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=195 ##   (At lambda=0.0095): ##     Nonzero coefficients: 45 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.42 ##     Signal-to-noise ratio: 0.72 ##     Scale estimate (sigma): 0.370 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    21        5      0.238       1 ##  Order 1 interaction   174       40      0.230       1 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=195 ##   (At lambda=0.0555): ##     Nonzero coefficients: 6 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.39 ##     Signal-to-noise ratio: 0.64 ##     Scale estimate (sigma): 0.380 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    21        3     0.1429       1 ##  Order 1 interaction   174        3     0.0172       1 ##  ## $ME ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=21 ##   (At lambda=0.0030): ##     Nonzero coefficients: 10 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.41 ##     Signal-to-noise ratio: 0.70 ##     Scale estimate (sigma): 0.373 ##  ##   SR information: ##      Vartype Total Selected Saturation Penalty ##  Main effect    21       10      0.476    4.58 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=21 ##   (At lambda=0.0158): ##     Nonzero coefficients: 3 ##     Cross-validation error (deviance): 0.15 ##     R-squared: 0.38 ##     Signal-to-noise ratio: 0.61 ##     Scale estimate (sigma): 0.383 ##  ##   SR information: ##      Vartype Total Selected Saturation Penalty ##  Main effect    21        3      0.143    4.58 ##  ## $SRLp ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=204 ##   (At lambda=0.0009): ##     Nonzero coefficients: 41 ##     Cross-validation error (deviance): 0.13 ##     R-squared: 0.43 ##     Signal-to-noise ratio: 0.75 ##     Scale estimate (sigma): 0.367 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    21       11      0.524    4.58 ##  Order 1 interaction   174       24      0.138   13.19 ##   Order 2 polynomial     9        6      0.667    5.48 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=770, p=204 ##   (At lambda=0.0093): ##     Nonzero coefficients: 4 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.40 ##     Signal-to-noise ratio: 0.66 ##     Scale estimate (sigma): 0.377 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    21        3      0.143    4.58 ##  Order 1 interaction   174        0      0.000   13.19 ##   Order 2 polynomial     9        1      0.111    5.48 n <- lapply(lso, plot, log.l = TRUE) mcp <- list(   SRM  = sparseR(CASE ~ ., train, seed = 1, penalty = \"MCP\"),            ## SRM model   APM  = sparseR(CASE ~ ., train, seed = 1, gamma = 0, penalty = \"MCP\"), ## APM model   MEM   = sparseR(CASE ~ ., train, seed = 1, k = 0, penalty = \"MCP\"),     ## Main effects MCP model   SRMp = sparseR(CASE ~ ., train, seed = 1, poly = 2, penalty = \"MCP\")   ## SRM + polynomials ) n <- lapply(mcp, plot, log.l = TRUE) scad <- list(   SRS  = sparseR(CASE ~ ., train, seed = 1, penalty = \"SCAD\"),            ## SRS model   APS  = sparseR(CASE ~ ., train, seed = 1, gamma = 0, penalty = \"SCAD\"), ## APS model   MES   = sparseR(CASE ~ ., train, seed = 1, k = 0, penalty = \"SCAD\"),    ## Main effects SCAD model   SRSp = sparseR(CASE ~ ., train, seed = 1, poly = 2, penalty = \"SCAD\")   ## SRS + polynomials )  n <- lapply(scad, plot, log.l = TRUE) lapply(lso, function(x) bind_rows(x$results_summary, x$results1se_summary)) lapply(mcp, function(x) bind_rows(x$results_summary, x$results1se_summary))"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"future-work","dir":"Articles","previous_headings":"","what":"Future work:","title":"Using the `sparseR` package","text":"section still incomplete active development.","code":""},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"use-case-heart-data","dir":"Articles","previous_headings":"Future work:","what":"Use case: heart data","title":"Using the `sparseR` package","text":"","code":"## Load Data set, correctly code factors + outcome data(\"Detrano\") cleveland$thal <- factor(cleveland$thal) cleveland$case <- 1*(cleveland$num > 0)  # Convert variables into factor variables if necessary! summary(cleveland) ##       age             sex               cp           trestbps     ##  Min.   :29.00   Min.   :0.0000   Min.   :1.000   Min.   : 94.0   ##  1st Qu.:48.00   1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:120.0   ##  Median :56.00   Median :1.0000   Median :3.000   Median :130.0   ##  Mean   :54.44   Mean   :0.6799   Mean   :3.158   Mean   :131.7   ##  3rd Qu.:61.00   3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:140.0   ##  Max.   :77.00   Max.   :1.0000   Max.   :4.000   Max.   :200.0   ##                                                                   ##       chol            fbs            restecg          thalach      ##  Min.   :126.0   Min.   :0.0000   Min.   :0.0000   Min.   : 71.0   ##  1st Qu.:211.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:133.5   ##  Median :241.0   Median :0.0000   Median :1.0000   Median :153.0   ##  Mean   :246.7   Mean   :0.1485   Mean   :0.9901   Mean   :149.6   ##  3rd Qu.:275.0   3rd Qu.:0.0000   3rd Qu.:2.0000   3rd Qu.:166.0   ##  Max.   :564.0   Max.   :1.0000   Max.   :2.0000   Max.   :202.0   ##                                                                    ##      exang           oldpeak         slope             ca           thal     ##  Min.   :0.0000   Min.   :0.00   Min.   :1.000   Min.   :0.0000   3   :166   ##  1st Qu.:0.0000   1st Qu.:0.00   1st Qu.:1.000   1st Qu.:0.0000   6   : 18   ##  Median :0.0000   Median :0.80   Median :2.000   Median :0.0000   7   :117   ##  Mean   :0.3267   Mean   :1.04   Mean   :1.601   Mean   :0.6722   NA's:  2   ##  3rd Qu.:1.0000   3rd Qu.:1.60   3rd Qu.:2.000   3rd Qu.:1.0000              ##  Max.   :1.0000   Max.   :6.20   Max.   :3.000   Max.   :3.0000              ##                                                  NA's   :4                   ##       num              case        ##  Min.   :0.0000   Min.   :0.0000   ##  1st Qu.:0.0000   1st Qu.:0.0000   ##  Median :0.0000   Median :0.0000   ##  Mean   :0.9373   Mean   :0.4587   ##  3rd Qu.:2.0000   3rd Qu.:1.0000   ##  Max.   :4.0000   Max.   :1.0000   ## sapply(cleveland, function(x) length(unique(x))) ##      age      sex       cp trestbps     chol      fbs  restecg  thalach  ##       41        2        4       50      152        2        3       91  ##    exang  oldpeak    slope       ca     thal      num     case  ##        2       40        3        5        4        5        2 cleveland$sex <- factor(cleveland$sex) cleveland$fbs <- factor(cleveland$fbs) cleveland$exang <- factor(cleveland$exang)  # Set seed for reproducibility set.seed(167)  # Split data into test and train N <- nrow(cleveland) trainIDX <- sample(1:N, N*.5) trainDF <- cleveland[trainIDX,] %>%   select(-num) testDF <- cleveland[-trainIDX,] %>%   select(-num)  # Simulate missing data trainDF$thal[2] <- trainDF$thalach[1] <- NA  lso <- list(   SRL  = sparseR(case ~ ., trainDF, seed = 1), ## SRL model   APL  = sparseR(case ~ ., trainDF, seed = 1, gamma = 0), ## APL model   ME   = sparseR(case ~ ., trainDF, seed = 1, k = 0), ## Main effects model   SRLp = sparseR(case ~ ., trainDF, seed = 1, poly = 2) ## SRL + polynomials )  lso ## $SRL ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=153 ##   (At lambda=0.0070): ##     Nonzero coefficients: 12 ##     Cross-validation error (deviance): 0.13 ##     R-squared: 0.48 ##     Signal-to-noise ratio: 0.93 ##     Scale estimate (sigma): 0.357 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    18       12      0.667    4.24 ##  Order 1 interaction   135        0      0.000   11.62 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=153 ##   (At lambda=0.0204): ##     Nonzero coefficients: 7 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.42 ##     Signal-to-noise ratio: 0.73 ##     Scale estimate (sigma): 0.377 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    18        7      0.389    4.24 ##  Order 1 interaction   135        0      0.000   11.62 ##  ## $APL ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=153 ##   (At lambda=0.0635): ##     Nonzero coefficients: 15 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.43 ##     Signal-to-noise ratio: 0.75 ##     Scale estimate (sigma): 0.375 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    18        6     0.3333       1 ##  Order 1 interaction   135        9     0.0667       1 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=153 ##   (At lambda=0.1144): ##     Nonzero coefficients: 9 ##     Cross-validation error (deviance): 0.15 ##     R-squared: 0.37 ##     Signal-to-noise ratio: 0.59 ##     Scale estimate (sigma): 0.393 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    18        5     0.2778       1 ##  Order 1 interaction   135        4     0.0296       1 ##  ## $ME ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=18 ##   (At lambda=0.0033): ##     Nonzero coefficients: 13 ##     Cross-validation error (deviance): 0.13 ##     R-squared: 0.49 ##     Signal-to-noise ratio: 0.96 ##     Scale estimate (sigma): 0.354 ##  ##   SR information: ##      Vartype Total Selected Saturation Penalty ##  Main effect    18       13      0.722    4.24 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=18 ##   (At lambda=0.0193): ##     Nonzero coefficients: 7 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.43 ##     Signal-to-noise ratio: 0.75 ##     Scale estimate (sigma): 0.375 ##  ##   SR information: ##      Vartype Total Selected Saturation Penalty ##  Main effect    18        7      0.389    4.24 ##  ## $SRLp ##  ## Model summary @ min CV: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=162 ##   (At lambda=0.0033): ##     Nonzero coefficients: 24 ##     Cross-validation error (deviance): 0.12 ##     R-squared: 0.51 ##     Signal-to-noise ratio: 1.04 ##     Scale estimate (sigma): 0.348 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    18       14     0.7778    4.24 ##  Order 1 interaction   135        7     0.0519   11.62 ##   Order 2 polynomial     9        3     0.3333    5.20 ##  ##  ## Model summary @ CV1se: ## ----------------------------------------------------- ##   lasso-penalized linear regression with n=151, p=162 ##   (At lambda=0.0156): ##     Nonzero coefficients: 8 ##     Cross-validation error (deviance): 0.14 ##     R-squared: 0.45 ##     Signal-to-noise ratio: 0.82 ##     Scale estimate (sigma): 0.368 ##  ##   SR information: ##              Vartype Total Selected Saturation Penalty ##          Main effect    18        8      0.444    4.24 ##  Order 1 interaction   135        0      0.000   11.62 ##   Order 2 polynomial     9        0      0.000    5.20 plot(lso$SRL) lapply(lso, plot, log.l = TRUE)"},{"path":"https://petersonr.github.io/sparseR/articles/sparseR.html","id":"use-case-shedden-survival-data","dir":"Articles","previous_headings":"Future work:","what":"Use case: Shedden survival data","title":"Using the `sparseR` package","text":"TBD","code":""},{"path":"https://petersonr.github.io/sparseR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ryan Andrew Peterson. Author, maintainer.","code":""},{"path":"https://petersonr.github.io/sparseR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Peterson, R.., Cavanaugh, J.E. Ranked sparsity: cogent regularization framework selecting estimating feature interactions polynomials. AStA Adv Stat Anal (2022). https://link.springer.com/article/10.1007/s10182-021-00431-7","code":"@Article{,   title = {Ranked Sparsity: A Cogent Regularization Framework for Selecting and Estimating Feature Interactions and Polynomials},   author = {Ryan A. Peterson and Joseph E. Cavanaugh},   year = {2022},   journal = {AStA Advances in Statistical Analysis},   doi = {10.1007/s10182-021-00431-7}, }"},{"path":[]},{"path":"https://petersonr.github.io/sparseR/index.html","id":"what-is-ranked-sparsity","dir":"","previous_headings":"","what":"What is ranked sparsity?","title":"Variable Selection under Ranked Sparsity Principles for Interactions and Polynomials","text":"ranked sparsity methods sparsity-ranked lasso (SRL) developed model selection estimation presence interactions polynomials (Peterson & Cavanaugh 2022)[https://doi.org/10.1007/s10182-021-00431-7]. main idea algorithm skeptical higher-order polynomials interactions priori compared main effects, predetermined amount.","code":""},{"path":"https://petersonr.github.io/sparseR/index.html","id":"package-overview","dir":"","previous_headings":"","what":"Package overview","title":"Variable Selection under Ranked Sparsity Principles for Interactions and Polynomials","text":"sparseR package implements ranked-sparsity-based versions lasso, elastic net, MCP, SCAD. also provide (preliminary) version sparsity-ranked extension Bayesian Information Criterion (corresponding stepwise approaches) Additionally, sparseR many features designed streamline dealing interaction terms polynomials, including functions variable pre-processing, variable selection, post-selection inference, post-fit model visualization ranked sparsity.","code":""},{"path":"https://petersonr.github.io/sparseR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Variable Selection under Ranked Sparsity Principles for Interactions and Polynomials","text":"","code":"## Via GitHub:  # install.packages(\"devtools\") devtools::install_github(\"petersonR/sparseR\")  # or via CRAN install.packages(\"sparseR\")"},{"path":"https://petersonr.github.io/sparseR/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Variable Selection under Ranked Sparsity Principles for Interactions and Polynomials","text":"examples closer look use package, check package website. Many thanks authors maintainers ncvreg recipes.","code":"library(sparseR) data(iris) set.seed(1321)  srl <- sparseR(Sepal.Width ~ ., data = iris, k = 1, seed = 1) srl #>  #> Model summary @ min CV: #> ----------------------------------------------------- #>   lasso-penalized linear regression with n=150, p=18 #>   (At lambda=0.0015): #>     Nonzero coefficients: 10 #>     Cross-validation error (deviance): 0.07 #>     R-squared: 0.62 #>     Signal-to-noise ratio: 1.64 #>     Scale estimate (sigma): 0.267 #>  #>   SR information: #>              Vartype Total Selected Saturation Penalty #>          Main effect     6        4      0.667    2.45 #>  Order 1 interaction    12        6      0.500    3.46 #>  #>  #> Model summary @ CV1se: #> ----------------------------------------------------- #>   lasso-penalized linear regression with n=150, p=18 #>   (At lambda=0.0070): #>     Nonzero coefficients: 7 #>     Cross-validation error (deviance): 0.08 #>     R-squared: 0.57 #>     Signal-to-noise ratio: 1.33 #>     Scale estimate (sigma): 0.285 #>  #>   SR information: #>              Vartype Total Selected Saturation Penalty #>          Main effect     6        3      0.500    2.45 #>  Order 1 interaction    12        4      0.333    3.46"},{"path":"https://petersonr.github.io/sparseR/reference/custom_ics.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom IC functions for stepwise models — EBIC","title":"Custom IC functions for stepwise models — EBIC","text":"Custom IC functions stepwise models","code":""},{"path":"https://petersonr.github.io/sparseR/reference/custom_ics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Custom IC functions for stepwise models — EBIC","text":"","code":"EBIC(...)  # S3 method for default EBIC(fit, varnames, pen_info, gammafn = NULL, return_df = TRUE, ...)  RBIC(fit, ...)  RAIC(fit, ...)"},{"path":"https://petersonr.github.io/sparseR/reference/custom_ics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Custom IC functions for stepwise models — EBIC","text":"... additional args fit fitted object varnames names variables pen_info penalty information gammafn use gamma formula return_df deg. freedom returned","code":""},{"path":"https://petersonr.github.io/sparseR/reference/custom_ics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Custom IC functions for stepwise models — EBIC","text":"vector values criterion requested, degrees   freedom (appended front vector) return_df == TRUE.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/datasets.html","id":null,"dir":"Reference","previous_headings":"","what":"Data sets — datasets","title":"Data sets — datasets","text":"Detrano data sets (cleveland, hungarian, switzerland, va); Iowa Radon Lung Cancer Study (irlcs_radon_syn): Data simulated resemble IRLCS study; Sheddon survival data (Z: clinical covariates, S:survival outcome)","code":""},{"path":"https://petersonr.github.io/sparseR/reference/datasets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data sets — datasets","text":"","code":"cleveland  hungarian  switzerland  va  irlcs_radon_syn  Z  S"},{"path":"https://petersonr.github.io/sparseR/reference/datasets.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data sets — datasets","text":"object class data.frame 303 rows 14 columns. object class data.frame 294 rows 14 columns. object class data.frame 123 rows 14 columns. object class data.frame 200 rows 14 columns. object class data.frame 1027 rows 16 columns. object class data.frame 442 rows 6 columns. object class Surv 442 rows 2 columns.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/datasets.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Data sets — datasets","text":"Detrano data https://archive.ics.uci.edu/ml/datasets/heart+disease IRLCS data sets https://cheec.uiowa.edu/research/residential-radon--lung-cancer-case-control-study Sheddon https://www.gsea-msigdb.org/gsea/msigdb/cards/SHEDDEN_LUNG_CANCER_POOR_SURVIVAL_A6","code":""},{"path":"https://petersonr.github.io/sparseR/reference/datasets.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Data sets — datasets","text":"Detrano Detrano, R., Janosi, ., Steinbrunn, W., Pfisterer, M., Schmid, J., Sandhu, S., Guppy, K.,   Lee, S., & Froelicher, V. (1989). International application new probability algorithm   diagnosis coronary artery disease. American Journal Cardiology, 64,304--310. IRLCS FIELD, R., SMITH, B., STECK, D. et al. Residential radon exposure lung cancer:     Variation risk estimates using alternative exposure scenarios. J Expo Sci Environ Epidemiol     12, 197–203 (2002). https://www.nature.com/articles/7500215 Shedden Director's Challenge Consortium Molecular Classification Lung Adenocarcinoma, Shedden, K.,   Taylor, J. M., Enkemann, S. ., Tsao, M. S., Yeatman, T. J., Gerald, W. L., Eschrich, S., Jurisica, .,   Giordano, T. J., Misek, D. E., Chang, . C., Zhu, C. Q., Strumpf, D., Hanash, S., Shepherd, F. ., Ding,   K., Seymour, L., Naoki, K., Pennell, N., … Beer, D. G. (2008). Gene expression-based survival prediction   lung adenocarcinoma: multi-site, blinded validation study. Nature medicine, 14(8), 822–827.   https://www.nature.com/articles/nm.1790","code":""},{"path":"https://petersonr.github.io/sparseR/reference/effect_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot relevant effects of a sparseR object — effect_plot","title":"Plot relevant effects of a sparseR object — effect_plot","text":"Plot relevant effects sparseR object","code":""},{"path":"https://petersonr.github.io/sparseR/reference/effect_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot relevant effects of a sparseR object — effect_plot","text":"","code":"effect_plot(fit, ...)  # S3 method for sparseR effect_plot(   fit,   coef_name,   at = c(\"cvmin\", \"cv1se\"),   by = NULL,   by_levels,   nn = 101,   plot.args = list(),   resids = TRUE,   ... )  # S3 method for sparseRBIC effect_plot(   fit,   coef_name,   by = NULL,   by_levels,   nn = 101,   plot.args = list(),   resids = TRUE,   ... )"},{"path":"https://petersonr.github.io/sparseR/reference/effect_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot relevant effects of a sparseR object — effect_plot","text":"fit `sparseR` object ... additional arguments coef_name name coefficient plot along x-axis value lambda use variable(s) involved (possible) interaction by_levels values cut continuous variable (defaults 3 quantiles) nn number points plot along prediction line plot.args list arguments passed plot resids residuals plotted ?","code":""},{"path":"https://petersonr.github.io/sparseR/reference/effect_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot relevant effects of a sparseR object — effect_plot","text":"nothing returned Nothing (invisible) returned","code":""},{"path":"https://petersonr.github.io/sparseR/reference/get_penalties.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to help set up penalties — get_penalties","title":"Helper function to help set up penalties — get_penalties","text":"Helper function help set penalties","code":""},{"path":"https://petersonr.github.io/sparseR/reference/get_penalties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to help set up penalties — get_penalties","text":"","code":"get_penalties(   varnames,   poly,   poly_prefix = \"poly_\",   int_sep = \"\\\\:\",   pool = FALSE,   gamma = 0.5,   cumulative_k = FALSE,   cumulative_poly = TRUE )"},{"path":"https://petersonr.github.io/sparseR/reference/get_penalties.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper function to help set up penalties — get_penalties","text":"varnames names covariates model matrix poly max polynomial considered poly_prefix comes polynomial specification varnames? int_sep denotes multiplication interactions? pool polynomials interactions pooled? gamma much penalty increase group size (0.5 assumes equal contribution prior information) cumulative_k penalties increased cumulatively order interaction increases? (used !pool) cumulative_poly penalties increased cumulatively order polynomial increases? (used !pool)","code":""},{"path":"https://petersonr.github.io/sparseR/reference/get_penalties.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper function to help set up penalties — get_penalties","text":"list relevant information variables, including: penalties numeric value penalties vartype Variable type (main effect, order k interaction, etc) varname names variables","code":""},{"path":"https://petersonr.github.io/sparseR/reference/get_penalties.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Helper function to help set up penalties — get_penalties","text":"primarily helper function sparseR, may   useful model matrix set hand.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://petersonr.github.io/sparseR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling `rhs(lhs)`.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/plot.sparseR.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot relevant properties of sparseR objects — plot.sparseR","title":"Plot relevant properties of sparseR objects — plot.sparseR","text":"Plot relevant properties sparseR objects","code":""},{"path":"https://petersonr.github.io/sparseR/reference/plot.sparseR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot relevant properties of sparseR objects — plot.sparseR","text":"","code":"# S3 method for sparseR plot(x, plot_type = c(\"both\", \"cv\", \"path\"), cols = NULL, log.l = TRUE, ...)"},{"path":"https://petersonr.github.io/sparseR/reference/plot.sparseR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot relevant properties of sparseR objects — plot.sparseR","text":"x `sparseR` object plot_type solution path, CV results, plotted? cols option specify color groups log.l x-axis (lambda) logged? ... extra plotting options","code":""},{"path":"https://petersonr.github.io/sparseR/reference/plot.sparseR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot relevant properties of sparseR objects — plot.sparseR","text":"nothing returned","code":""},{"path":"https://petersonr.github.io/sparseR/reference/predict.sparseR.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict coefficients or responses for sparseR object — predict.sparseR","title":"Predict coefficients or responses for sparseR object — predict.sparseR","text":"Predict coefficients responses sparseR object","code":""},{"path":"https://petersonr.github.io/sparseR/reference/predict.sparseR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict coefficients or responses for sparseR object — predict.sparseR","text":"","code":"# S3 method for sparseR predict(object, newdata, lambda, at = c(\"cvmin\", \"cv1se\"), ...)  # S3 method for sparseR coef(object, lambda, at = c(\"cvmin\", \"cv1se\"), ...)"},{"path":"https://petersonr.github.io/sparseR/reference/predict.sparseR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict coefficients or responses for sparseR object — predict.sparseR","text":"object sparseR object newdata new data make predictions lambda particular value lambda predict \"smart\" guess use lambda ... additional arguments passed predict.ncvreg","code":""},{"path":"https://petersonr.github.io/sparseR/reference/predict.sparseR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict coefficients or responses for sparseR object — predict.sparseR","text":"predicted outcomes `newdata` (coefficients)   specified (smart) lambda value","code":""},{"path":"https://petersonr.github.io/sparseR/reference/print.sparseR.html","id":null,"dir":"Reference","previous_headings":"","what":"Print sparseR object — print.sparseR","title":"Print sparseR object — print.sparseR","text":"Print sparseR object","code":""},{"path":"https://petersonr.github.io/sparseR/reference/print.sparseR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print sparseR object — print.sparseR","text":"","code":"# S3 method for sparseR print(x, prep = FALSE, ...)"},{"path":"https://petersonr.github.io/sparseR/reference/print.sparseR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print sparseR object — print.sparseR","text":"x sparseR object prep SR set-information printed well? ... additional arguments passed print.ncvreg","code":""},{"path":"https://petersonr.github.io/sparseR/reference/print.sparseR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print sparseR object — print.sparseR","text":"returns x invisibly","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"sparseR: Implement ranked sparsity for selecting interactions and polynomials — sparseR-package","title":"sparseR: Implement ranked sparsity for selecting interactions and polynomials — sparseR-package","text":"sparseR package implements various techniques selecting set interaction polynomial terms ranked sparsity. Additional tools data pre-processing, post-selection inference, visualization also included.","code":""},{"path":[]},{"path":"https://petersonr.github.io/sparseR/reference/sparseR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"sparseR: Implement ranked sparsity for selecting interactions and polynomials — sparseR-package","text":"Maintainer: Ryan Andrew Peterson ryan..peterson@cuanschutz.edu (ORCID)","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a ranked-sparsity model with regularized regression — sparseR","title":"Fit a ranked-sparsity model with regularized regression — sparseR","text":"Fit ranked-sparsity model regularized regression","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a ranked-sparsity model with regularized regression — sparseR","text":"","code":"sparseR(   formula,   data,   family = c(\"gaussian\", \"binomial\", \"poisson\", \"coxph\"),   penalty = c(\"lasso\", \"MCP\", \"SCAD\"),   alpha = 1,   ncvgamma = 3,   lambda.min = 0.005,   k = 1,   poly = 1,   gamma = 0.5,   cumulative_k = FALSE,   cumulative_poly = TRUE,   pool = FALSE,   ia_formula = NULL,   pre_process = TRUE,   model_matrix = NULL,   y = NULL,   poly_prefix = \"_poly_\",   int_sep = \"\\\\:\",   pre_proc_opts = c(\"knnImpute\", \"scale\", \"center\", \"otherbin\", \"none\"),   filter = c(\"nzv\", \"zv\"),   extra_opts = list(),   ... )"},{"path":"https://petersonr.github.io/sparseR/reference/sparseR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a ranked-sparsity model with regularized regression — sparseR","text":"formula Names terms data Data family family model penalty penalty used (lasso, MCP, SCAD) alpha mix L1 penalty (lower values introduce L2 ridge penalty) ncvgamma tuning parameter ncvreg (MCP SCAD) lambda.min minimum value used lambda (ratio max, see ?ncvreg) k maximum order interactions consider poly maximum order polynomials consider gamma degree extremity sparsity rankings (see details) cumulative_k penalties increased cumulatively order interaction increases? cumulative_poly penalties increased cumulatively order polynomial increases? pool interactions order k polynomials order k+1 pooled together calculating penalty? ia_formula formula passed step_interact (interactions, see details) pre_process data preprocessed (FALSE, must provide model_matrix) model_matrix data frame matrix specifying full model matrix (used !pre_process) y vector responses (used !pre_process) poly_prefix model_matrix specified, prefix polynomial terms? int_sep model_matrix specified, separator interaction terms? pre_proc_opts List preprocessing steps (see details) filter type filter applied main effects + interactions extra_opts list options preprocess steps (see details) ... Additional arguments (passed fitting function)","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a ranked-sparsity model with regularized regression — sparseR","text":"object class sparseR containing following: fit fit object returned ncvreg srprep recipes object used prep data pen_factors factor multiple penalties ranked sparsity results coefficients penalty factors minimum CV lambda results_summary tibble summary results minimum CV lambda results1se coefficients penalty factors lambda_1se results1se_summary tibble summary results lambda_1se data (unprocessed) data family family argument (non-normal, eg. poisson) info list containing meta-info procedure","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a ranked-sparsity model with regularized regression — sparseR","text":"Selecting gamma: higher values gamma penalize \"group\" size . default, set 0.5, yields equal contribution prior information across orders interactions/polynomials (good default settings). Additionally, setting cumulative_poly cumulative_k TRUE increases penalty cumulatively based order either polynomial interaction. options can passed pre_proc_opts : - knnImpute (missing data imputed?) - scale (data standardized)? - center (data centered mean another value?) - otherbin (factors low prevalence combined?) - none (preprocessing done? can also specify null object) options can passed extra_opts : - centers (named numeric vector denotes covariate centered) - center_fn (alternatively, function can specified calculate center min median) - freq_cut, unique_cut (see ?step_nzv - get used filtering steps) - neighbors (number neighbors knnImpute) - one_hot (see ?step_dummy), defaults cell-means coding can done regularized regression (change risk) - raw (polynomials orthogonal? defaults true variables centered scaled already point default) ia_formula default interact variables order k. specified, ia_formula passed terms argument recipes::step_interact, help documentation function can investigated assistance specifying specific interactions.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit a ranked-sparsity model with regularized regression — sparseR","text":"fitting functionality, ncvreg package used; see Breheny, P. Huang, J. (2011) Coordinate descent algorithms nonconvex penalized regression, applications biological feature selection. Ann. Appl. Statist., 5: 232-253.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap procedure for stepwise regression — sparseRBIC_bootstrap","title":"Bootstrap procedure for stepwise regression — sparseRBIC_bootstrap","text":"Runs bootstrap models selection procedure using RBIC find bootstrapped standard error (smoothed, see Efron 2014) well selection percentage across candidate variables. (experimental)","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap procedure for stepwise regression — sparseRBIC_bootstrap","text":"","code":"sparseRBIC_bootstrap(srbic_fit, B = 100, quiet = FALSE)"},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap procedure for stepwise regression — sparseRBIC_bootstrap","text":"srbic_fit object fitted sparseRBIC_step B Number bootstrap samples quiet display progress bar silenced?","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap procedure for stepwise regression — sparseRBIC_bootstrap","text":"list containing: results tibble containing coefficients, p-values, selection pct bootstraps tibble bootstrapped coefficients","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_sampsplit.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample split procedure for stepwise regression — sparseRBIC_sampsplit","title":"Sample split procedure for stepwise regression — sparseRBIC_sampsplit","text":"Runs multiple models selection procedures using RBIC achieve valid inferential results post-selection","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_sampsplit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample split procedure for stepwise regression — sparseRBIC_sampsplit","text":"","code":"sparseRBIC_sampsplit(srbic_fit, S = 100, quiet = FALSE)"},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_sampsplit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample split procedure for stepwise regression — sparseRBIC_sampsplit","text":"srbic_fit object fitted sparseRBIC_step S Number splitting iterations quiet display progress bar silenced?","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_sampsplit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample split procedure for stepwise regression — sparseRBIC_sampsplit","text":"list containing: results tibble containing coefficients, p-values, selection pct splits tibble different split-based coefficients","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_step.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a ranked-sparsity model with forward stepwise RBIC (experimental) — sparseRBIC_step","title":"Fit a ranked-sparsity model with forward stepwise RBIC (experimental) — sparseRBIC_step","text":"Fit ranked-sparsity model forward stepwise RBIC (experimental)","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_step.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a ranked-sparsity model with forward stepwise RBIC (experimental) — sparseRBIC_step","text":"","code":"sparseRBIC_step(   formula,   data,   family = c(\"gaussian\", \"binomial\", \"poisson\"),   k = 1,   poly = 1,   ic = c(\"RBIC\", \"RAIC\", \"BIC\", \"AIC\", \"EBIC\"),   hier = c(\"strong\", \"weak\", \"none\"),   sequential = (hier[1] != \"none\"),   cumulative_k = FALSE,   cumulative_poly = TRUE,   pool = FALSE,   ia_formula = NULL,   pre_process = TRUE,   model_matrix = NULL,   y = NULL,   poly_prefix = \"_poly_\",   int_sep = \"\\\\:\",   pre_proc_opts = c(\"knnImpute\", \"scale\", \"center\", \"otherbin\", \"none\"),   filter = c(\"nzv\", \"zv\"),   extra_opts = list(),   trace = 0,   message = TRUE,   ... )"},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_step.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a ranked-sparsity model with forward stepwise RBIC (experimental) — sparseRBIC_step","text":"formula Names terms data Data family family model k maximum order interactions consider poly maximum order polynomials consider ic information criterion use hier hierarchy enforced (weak strong)? Must set sequential == TRUE (see details) sequential main effects considered first, orders sequentially added/considered? cumulative_k penalties increased cumulatively order interaction increases? cumulative_poly penalties increased cumulatively order polynomial increases? pool interactions order k polynomials order k+1 pooled together calculating penalty? ia_formula formula passed step_interact via terms argument pre_process data preprocessed (FALSE, must provide model_matrix) model_matrix data frame matrix specifying full model matrix (used !pre_process) y vector responses (used !pre_process) poly_prefix model_matrix specified, prefix polynomial terms? int_sep model_matrix specified, separator interaction terms? pre_proc_opts List preprocessing steps (see details) filter type filter applied main effects + interactions extra_opts list options preprocess steps (see details) trace intermediate results model selection process output message experimental message suppressed ... additional arguments running stepwise selection","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_step.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a ranked-sparsity model with forward stepwise RBIC (experimental) — sparseRBIC_step","text":"object class sparseRBIC containing following: fit final fit object srprep recipes object used prep data pen_info coefficient-level variable counts, types + names data (unprocessed) data family family argument (non-normal, eg. poisson) info list containing meta-info procedure stats IC fit respective terms included","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseRBIC_step.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a ranked-sparsity model with forward stepwise RBIC (experimental) — sparseRBIC_step","text":"function mirrors sparseR uses stepwise selection guided RBIC. Additionally, setting cumulative_poly cumulative_k TRUE increases penalty cumulatively based order either polynomial interaction. hier hierarchy enforcement work sequential == TRUE, notably consider \"first gen\" hierarchy, , main effects make interaction already model. therefore possible third order interaction (x1:x2:x3) enter model without x1:x2 x2:x3, long x1, x2, x3 model. options can passed pre_proc_opts : knnImpute (missing data imputed?) scale (data standardized)? center (data centered mean another value?) otherbin (factors low prevalence combined?) none (preprocessing done? can also specify null object) options can passed extra_opts : centers (named numeric vector denotes covariate centered) center_fn (alternatively, function can specified calculate center min median) freq_cut, unique_cut (see ?step_nzv - get used filtering steps) neighbors (number neighbors knnImpute) one_hot (see ?step_dummy), defaults cell-means coding can done regularized regression (change risk) raw (polynomials orthogonal? defaults true variables centered scaled already point default)","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR_prep.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess & create a model matrix with interactions + polynomials — sparseR_prep","title":"Preprocess & create a model matrix with interactions + polynomials — sparseR_prep","text":"Preprocess & create model matrix interactions + polynomials","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR_prep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess & create a model matrix with interactions + polynomials — sparseR_prep","text":"","code":"sparseR_prep(   formula,   data,   k = 1,   poly = 1,   pre_proc_opts = c(\"knnImpute\", \"scale\", \"center\", \"otherbin\", \"none\"),   ia_formula = NULL,   filter = c(\"nzv\", \"zv\"),   extra_opts = list(),   family = \"gaussian\" )"},{"path":"https://petersonr.github.io/sparseR/reference/sparseR_prep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess & create a model matrix with interactions + polynomials — sparseR_prep","text":"formula formula main effects + outcome model data required data frame tibble containing variables formula k Maximum order interactions numeric variables poly maximum order polynomials consider pre_proc_opts character vector specifying methods preprocessing (see details) ia_formula formula passed step_interact (interactions, see details) filter methods used filter variables (near) zero variance? (see details) extra_opts extra options used preprocessing family family passed sparseR","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR_prep.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess & create a model matrix with interactions + polynomials — sparseR_prep","text":"object class recipe; see recipes::recipe()","code":""},{"path":"https://petersonr.github.io/sparseR/reference/sparseR_prep.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess & create a model matrix with interactions + polynomials — sparseR_prep","text":"pre_proc_opts acts wrapper corresponding procedures recipes package. currently supported options can passed pre_proc_opts : knnImpute: k-nearest-neighbors performed (necessary?) scale: variables scaled prior creating interactions (scale factor variables dummy variables) center: variables centered (center factor variables dummy variables ) otherbin: ia_formula default interact variables order k. specified, ia_formula passed terms argument recipes::step_interact, help documentation function can investigated assistance specifying specific interactions. methods specified filter important; filtering necessary cut extraneous polynomials interactions (cases really make sense). true, instance, using dummy variables polynomials , using interactions dummy variables relate categorical variable.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/step_center_to.html","id":null,"dir":"Reference","previous_headings":"","what":"Centering numeric data to a value besides their mean — step_center_to","title":"Centering numeric data to a value besides their mean — step_center_to","text":"`step_center_to` generalizes `step_center` allow different function `mean` function calculate centers. creates *specification* recipe step normalize numeric data 'center' zero.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/step_center_to.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Centering numeric data to a value besides their mean — step_center_to","text":"","code":"step_center_to(   recipe,   ...,   role = NA,   trained = FALSE,   centers = NULL,   center_fn = mean,   na_rm = TRUE,   skip = FALSE,   id = rand_id(\"center_to\") )  # S3 method for step_center_to tidy(x, ...)"},{"path":"https://petersonr.github.io/sparseR/reference/step_center_to.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Centering numeric data to a value besides their mean — step_center_to","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See [selections()] details. `tidy` method, currently used. role used step since new variables created. trained logical indicate quantities preprocessing estimated. centers named numeric vector centers. `NULL` computed [prep.recipe()] (can specified named numeric vector well?). center_fn function used calculate center na_rm logical value indicating whether `NA` values removed computations. skip logical. step skipped recipe baked [bake.recipe()]? operations baked [prep.recipe()] run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using `skip = TRUE` may affect computations subsequent operations id character string unique step identify . x `step_center_to` object.","code":""},{"path":"https://petersonr.github.io/sparseR/reference/step_center_to.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Centering numeric data to a value besides their mean — step_center_to","text":"updated version `recipe` new step added   sequence existing steps (). `tidy` method, tibble   columns `terms` (selectors variables selected) `value` (  centers).","code":""},{"path":"https://petersonr.github.io/sparseR/reference/step_center_to.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Centering numeric data to a value besides their mean — step_center_to","text":"Centering data means average variable subtracted   data. `step_center_to` estimates variable centers   data used `training` argument `prep.recipe`. `bake.recipe`   applies centering new data sets using centers.","code":""},{"path":[]},{"path":"https://petersonr.github.io/sparseR/reference/step_center_to.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Centering numeric data to a value besides their mean — step_center_to","text":"","code":"data(biomass, package = \"modeldata\")  biomass_tr <- biomass[biomass$dataset == \"Training\",] biomass_te <- biomass[biomass$dataset == \"Testing\",]  rec <- recipes::recipe(  HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur,  data = biomass_tr)  center_trans <- rec %>%   step_center_to(carbon, contains(\"gen\"), -hydrogen)  center_obj <- recipes::prep(center_trans, training = biomass_tr)  transformed_te <- recipes::bake(center_obj, biomass_te)  biomass_te[1:10, names(transformed_te)] #>    carbon hydrogen oxygen nitrogen sulfur    HHV #> 15  46.35     5.67  47.20     0.30   0.22 18.275 #> 20  43.25     5.50  48.06     2.85   0.34 17.560 #> 26  42.70     5.50  49.10     2.40   0.30 17.173 #> 31  46.40     6.10  37.30     1.80   0.50 18.851 #> 36  48.76     6.32  42.77     0.20   0.00 20.547 #> 41  44.30     5.50  41.70     0.70   0.20 18.467 #> 46  38.94     5.23  54.13     1.19   0.51 15.095 #> 51  42.10     4.66  33.80     0.95   0.20 16.240 #> 55  29.20     4.40  31.10     0.14   4.90 11.147 #> 65  27.80     3.77  23.69     4.63   1.05 10.750 transformed_te #> # A tibble: 80 × 6 #>     carbon hydrogen oxygen nitrogen sulfur   HHV #>      <dbl>    <dbl>  <dbl>    <dbl>  <dbl> <dbl> #>  1  -2.00      5.67   8.68   -0.775   0.22  18.3 #>  2  -5.10      5.5    9.54    1.78    0.34  17.6 #>  3  -5.65      5.5   10.6     1.33    0.3   17.2 #>  4  -1.95      6.1   -1.22    0.725   0.5   18.9 #>  5   0.406     6.32   4.25   -0.875   0     20.5 #>  6  -4.05      5.5    3.18   -0.375   0.2   18.5 #>  7  -9.41      5.23  15.6     0.115   0.51  15.1 #>  8  -6.25      4.66  -4.72   -0.125   0.2   16.2 #>  9 -19.2       4.4   -7.42   -0.935   4.9   11.1 #> 10 -20.6       3.77 -14.8     3.56    1.05  10.8 #> # ℹ 70 more rows  recipes::tidy(center_trans) #> # A tibble: 1 × 6 #>   number operation type      trained skip  id              #>    <int> <chr>     <chr>     <lgl>   <lgl> <chr>           #> 1      1 step      center_to FALSE   FALSE center_to_DwRQ3 recipes::tidy(center_obj) #> # A tibble: 1 × 6 #>   number operation type      trained skip  id              #>    <int> <chr>     <chr>     <lgl>   <lgl> <chr>           #> 1      1 step      center_to TRUE    FALSE center_to_DwRQ3"},{"path":"https://petersonr.github.io/sparseR/reference/summary.sparseR.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary of sparseR model coefficients — summary.sparseR","title":"Summary of sparseR model coefficients — summary.sparseR","text":"Summary sparseR model coefficients","code":""},{"path":"https://petersonr.github.io/sparseR/reference/summary.sparseR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary of sparseR model coefficients — summary.sparseR","text":"","code":"# S3 method for sparseR summary(object, lambda, at = c(\"cvmin\", \"cv1se\"), ...)"},{"path":"https://petersonr.github.io/sparseR/reference/summary.sparseR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary of sparseR model coefficients — summary.sparseR","text":"object sparseR object lambda particular value lambda predict \"smart\" guess use lambda ... additional arguments passed summary.ncvreg","code":""},{"path":"https://petersonr.github.io/sparseR/reference/summary.sparseR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary of sparseR model coefficients — summary.sparseR","text":"object class `summary.ncvreg` specified smart value   lambda.","code":""}]
